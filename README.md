# xxxBench

This repository contains the code of xxxBench, involving classification, detection, segmentation, and monocular depth estimation (MDE).

# Outline

# Features

# Installation

# Quick Start
Demo for each task

# Supported Tasks
<table>
    <tr>
        <td>Task</td>
        <td>Model</td>
        <td>Paper</td>
    </tr>
    <tr>
        <td rowspan="8">MDE</td>
        <td>Mono2</td>
        <td><a href=\"https://openaccess.thecvf.com/content_ICCV_2019/papers/Godard_Digging_Into_Self-Supervised_Monocular_Depth_Estimation_ICCV_2019_paper.pdf" rel=\"nofollow\">Digging Into Self-supervised Monocular Depth Estimation</a> ICCV 2019 </td>
    </tr>
    <tr>
        <td>Manydepth</td>
        <td>The Temporal Opportunist: Self-supervised Multi-frame Monocular Depth CVPR 2021</td>
    </tr>
    <tr>
        <td>Depthhint</td>
        <td>Self-supervised Monocular Depth Hints ICCV 2019</td>
    </tr>
    <tr>
        <td>Adabins</td>
        <td>Adabins: Depth Estimation Using Adaptive Bins CVPR 2021</td>
    </tr>
    <tr>
        <td>Midas</td>
        <td>Towards Robust Monocular Depth Estimation: Mixing Datasets For Zero-shot Cross-dataset Transfer TPAMI 2020</td>
    </tr>
    <tr>
        <td>DPT</td>
        <td>Dinov2: Learning Robust Visual Features Without Supervision arXiv 2023</td>
    </tr>
    <tr>
        <td>GLPN</td>
        <td>Global-local Path Networks For Monocular Depth Estimation With Vertical Cutdepth arXiv 2022</td>
    </tr>
    <tr>
        <td>DepthAnything</td>
        <td>Depth Anything: Unleashing The Power of Large-scale Unlabeled Data CVPR 2024</td>
    </tr>
    
</table>
